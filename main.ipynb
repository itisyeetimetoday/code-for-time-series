{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import random\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 40, 130)\n",
    "x1 = 2*np.sin(x)\n",
    "x2 = 2*np.cos(x)\n",
    " \n",
    "## random formula to generate input sequences\n",
    "y1 = 1.6*x1**4 - 2*x2 - 10\n",
    "y2 = 1.2*x2**2*x1 + 6*x2 - 6*x1\n",
    "y3 = 2*x1**3 + 2*x2**3 - x1*x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 15\n",
    "output_seq_len = 20\n",
    "x = np.linspace(0, 40, 130)\n",
    "train_data_x = x[:(130 - output_seq_len)]\n",
    " \n",
    "def true_output_signals(x):\n",
    "    x1 = 2 * np.sin(x)\n",
    "    x2 = 2 * np.cos(x)\n",
    "    return x1, x2\n",
    " \n",
    "def true_input_signals(x):\n",
    "    x1, x2 = true_output_signals(x)\n",
    "    y1 = 1.6*x1**4 - 2*x2 - 10\n",
    "    y2 = 1.2*x2**2 * x1 + 2*x2*3 - x1*6\n",
    "    y3 = 2*x1**3 + 2*x2**3 - x1*x2\n",
    "    return y1, y2, y3\n",
    " \n",
    "def noise_func(x, noise_factor = 2):\n",
    "    return np.random.randn(len(x)) * noise_factor\n",
    " \n",
    "def generate_samples_for_output(x):\n",
    "    x1, x2 = true_output_signals(x)\n",
    "    return x1+noise_func(x1, 0.5), \\\n",
    "           x2+noise_func(x2, 0.5)\n",
    " \n",
    "def generate_samples_for_input(x):\n",
    "    y1, y2, y3 = true_input_signals(x)\n",
    "    return y1+noise_func(y1, 2), \\\n",
    "           y2+noise_func(y2, 2), \\\n",
    "           y3+noise_func(y3, 2)\n",
    " \n",
    "def generate_train_samples(x = train_data_x, batch_size = 10):\n",
    " \n",
    "    total_start_points = len(x) - input_seq_len - output_seq_len\n",
    "    start_x_idx = np.random.choice(range(total_start_points), batch_size)\n",
    " \n",
    "    input_seq_x = [x[i:(i+input_seq_len)] for i in start_x_idx]\n",
    "    output_seq_x = [x[(i+input_seq_len):(i+input_seq_len+output_seq_len)] for i in start_x_idx]\n",
    " \n",
    "    input_seq_y = [generate_samples_for_input(x) for x in input_seq_x]\n",
    "    output_seq_y = [generate_samples_for_output(x) for x in output_seq_x]\n",
    " \n",
    "    ## return shape: (batch_size, time_steps, feature_dims)\n",
    "    return np.array(input_seq_y).transpose(0, 2, 1), np.array(output_seq_y).transpose(0, 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.framework import dtypes\n",
    " \n",
    "## Parameters\n",
    "learning_rate = 0.01\n",
    "lambda_l2_reg = 0.003 \n",
    " \n",
    "## Network Parameters\n",
    "# length of input signals\n",
    "input_seq_len = 15\n",
    "# length of output signals\n",
    "output_seq_len = 20\n",
    "# size of LSTM Cell\n",
    "hidden_dim = 64\n",
    "# num of input signals\n",
    "input_dim = 3\n",
    "# num of output signals\n",
    "output_dim = 2\n",
    "# num of stacked lstm layers\n",
    "num_stacked_layers = 2\n",
    "# gradient clipping - to avoid gradient exploding\n",
    "GRADIENT_CLIPPING = 2.5\n",
    " \n",
    "def build_graph(feed_previous = False):\n",
    " \n",
    "    tf.reset_default_graph()\n",
    " \n",
    "    global_step = tf.Variable(\n",
    "                  initial_value=0,\n",
    "                  name=\"global_step\",\n",
    "                  trainable=False,\n",
    "                  collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    " \n",
    "    weights = {\n",
    "        'out': tf.get_variable('Weights_out', \\\n",
    "                               shape = [hidden_dim, output_dim], \\\n",
    "                               dtype = tf.float32, \\\n",
    "                               initializer = tf.truncated_normal_initializer()),\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.get_variable('Biases_out', \\\n",
    "                               shape = [output_dim], \\\n",
    "                               dtype = tf.float32, \\\n",
    "                               initializer = tf.constant_initializer(0.)),\n",
    "    }\n",
    " \n",
    "    with tf.variable_scope('Seq2seq'):\n",
    "        # Encoder: inputs\n",
    "        enc_inp = [\n",
    "            tf.placeholder(tf.float32, shape=(None, input_dim), name=\"inp_{}\".format(t))\n",
    "               for t in range(input_seq_len)\n",
    "        ]\n",
    " \n",
    "        # Decoder: target outputs\n",
    "        target_seq = [\n",
    "            tf.placeholder(tf.float32, shape=(None, output_dim), name=\"y\".format(t))\n",
    "              for t in range(output_seq_len)\n",
    "        ]\n",
    " \n",
    "        # Give a \"GO\" token to the decoder.\n",
    "        # If dec_inp are fed into decoder as inputs, this is 'guided' training; otherwise only the\n",
    "        # first element will be fed as decoder input which is then 'un-guided'\n",
    "        dec_inp = [ tf.zeros_like(target_seq[0], dtype=tf.float32, name=\"GO\") ] + target_seq[:-1]\n",
    " \n",
    "        with tf.variable_scope('LSTMCell'):\n",
    "            cells = []\n",
    "            for i in range(num_stacked_layers):\n",
    "                with tf.variable_scope('RNN_{}'.format(i)):\n",
    "                    cells.append(tf.contrib.rnn.LSTMCell(hidden_dim))\n",
    "            cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    " \n",
    "        def _rnn_decoder(decoder_inputs,\n",
    "                        initial_state,\n",
    "                        cell,\n",
    "                        loop_function=None,\n",
    "                        scope=None):\n",
    "          \"\"\"RNN decoder for the sequence-to-sequence model.\n",
    "          Args:\n",
    "            decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "            initial_state: 2D Tensor with shape [batch_size x cell.state_size].\n",
    "            cell: rnn_cell.RNNCell defining the cell function and size.\n",
    "            loop_function: If not None, this function will be applied to the i-th output\n",
    "              in order to generate the i+1-st input, and decoder_inputs will be ignored,\n",
    "              except for the first element (\"GO\" symbol). This can be used for decoding,\n",
    "              but also for training to emulate http://arxiv.org/abs/1506.03099.\n",
    "              Signature -- loop_function(prev, i) = next\n",
    "                * prev is a 2D Tensor of shape [batch_size x output_size],\n",
    "                * i is an integer, the step number (when advanced control is needed),\n",
    "                * next is a 2D Tensor of shape [batch_size x input_size].\n",
    "            scope: VariableScope for the created subgraph; defaults to \"rnn_decoder\".\n",
    "          Returns:\n",
    "            A tuple of the form (outputs, state), where:\n",
    "              outputs: A list of the same length as decoder_inputs of 2D Tensors with\n",
    "                shape [batch_size x output_size] containing generated outputs.\n",
    "              state: The state of each cell at the final time-step.\n",
    "                It is a 2D Tensor of shape [batch_size x cell.state_size].\n",
    "                (Note that in some cases, like basic RNN cell or GRU cell, outputs and\n",
    "                 states can be the same. They are different for LSTM cells though.)\n",
    "          \"\"\"\n",
    "          with variable_scope.variable_scope(scope or \"rnn_decoder\"):\n",
    "            state = initial_state\n",
    "            outputs = []\n",
    "            prev = None\n",
    "            for i, inp in enumerate(decoder_inputs):\n",
    "              if loop_function is not None and prev is not None:\n",
    "                with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "                  inp = loop_function(prev, i)\n",
    "              if i > 0:\n",
    "                variable_scope.get_variable_scope().reuse_variables()\n",
    "              output, state = cell(inp, state)\n",
    "              outputs.append(output)\n",
    "              if loop_function is not None:\n",
    "                prev = output\n",
    "          return outputs, state\n",
    " \n",
    "        def _basic_rnn_seq2seq(encoder_inputs,\n",
    "                              decoder_inputs,\n",
    "                              cell,\n",
    "                              feed_previous,\n",
    "                              dtype=dtypes.float32,\n",
    "                              scope=None):\n",
    "          \"\"\"Basic RNN sequence-to-sequence model.\n",
    "          This model first runs an RNN to encode encoder_inputs into a state vector,\n",
    "          then runs decoder, initialized with the last encoder state, on decoder_inputs.\n",
    "          Encoder and decoder use the same RNN cell type, but don't share parameters.\n",
    "          Args:\n",
    "            encoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "            decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "            feed_previous: Boolean; if True, only the first of decoder_inputs will be\n",
    "              used (the \"GO\" symbol), all other inputs will be generated by the previous\n",
    "              decoder output using _loop_function below. If False, decoder_inputs are used\n",
    "              as given (the standard decoder case).\n",
    "            dtype: The dtype of the initial state of the RNN cell (default: tf.float32).\n",
    "            scope: VariableScope for the created subgraph; default: \"basic_rnn_seq2seq\".\n",
    "          Returns:\n",
    "            A tuple of the form (outputs, state), where:\n",
    "              outputs: A list of the same length as decoder_inputs of 2D Tensors with\n",
    "                shape [batch_size x output_size] containing the generated outputs.\n",
    "              state: The state of each decoder cell in the final time-step.\n",
    "                It is a 2D Tensor of shape [batch_size x cell.state_size].\n",
    "          \"\"\"\n",
    "          with variable_scope.variable_scope(scope or \"basic_rnn_seq2seq\"):\n",
    "            enc_cell = copy.deepcopy(cell)\n",
    "            _, enc_state = rnn.static_rnn(enc_cell, encoder_inputs, dtype=dtype)\n",
    "            if feed_previous:\n",
    "                return _rnn_decoder(decoder_inputs, enc_state, cell, _loop_function)\n",
    "            else:\n",
    "                return _rnn_decoder(decoder_inputs, enc_state, cell)\n",
    " \n",
    "        def _loop_function(prev, _):\n",
    "          '''Naive implementation of loop function for _rnn_decoder. Transform prev from\n",
    "          dimension [batch_size x hidden_dim] to [batch_size x output_dim], which will be\n",
    "          used as decoder input of next time step '''\n",
    "          return tf.matmul(prev, weights['out']) + biases['out']\n",
    " \n",
    "        dec_outputs, dec_memory = _basic_rnn_seq2seq(\n",
    "            enc_inp,\n",
    "            dec_inp,\n",
    "            cell,\n",
    "            feed_previous = feed_previous\n",
    "        )\n",
    " \n",
    "        reshaped_outputs = [tf.matmul(i, weights['out']) + biases['out'] for i in dec_outputs]\n",
    " \n",
    "    # Training loss and optimizer\n",
    "    with tf.variable_scope('Loss'):\n",
    "        # L2 loss\n",
    "        output_loss = 0\n",
    "        for _y, _Y in zip(reshaped_outputs, target_seq):\n",
    "            output_loss += tf.reduce_mean(tf.pow(_y - _Y, 2))\n",
    " \n",
    "        # L2 regularization for weights and biases\n",
    "        reg_loss = 0\n",
    "        for tf_var in tf.trainable_variables():\n",
    "            if 'Biases_' in tf_var.name or 'Weights_' in tf_var.name:\n",
    "                reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    " \n",
    "        loss = output_loss + lambda_l2_reg * reg_loss\n",
    " \n",
    "    with tf.variable_scope('Optimizer'):\n",
    "        optimizer = tf.contrib.layers.optimize_loss(\n",
    "                loss=loss,\n",
    "                learning_rate=learning_rate,\n",
    "                global_step=global_step,\n",
    "                optimizer='Adam',\n",
    "                clip_gradients=GRADIENT_CLIPPING)\n",
    " \n",
    "    saver = tf.train.Saver\n",
    " \n",
    "    return dict(\n",
    "        enc_inp = enc_inp,\n",
    "        target_seq = target_seq,\n",
    "        train_op = optimizer,\n",
    "        loss=loss,\n",
    "        saver = saver,\n",
    "        reshaped_outputs = reshaped_outputs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.316757\n",
      "34.1095\n",
      "62.265846\n",
      "16.516129\n",
      "15.834692\n",
      "12.3174515\n",
      "8.882545\n",
      "7.431957\n",
      "6.2552385\n",
      "7.3338923\n",
      "6.906034\n",
      "5.970617\n",
      "7.102262\n",
      "6.578037\n",
      "6.4012737\n",
      "6.089286\n",
      "6.717519\n",
      "5.9603877\n",
      "5.6086054\n",
      "6.231004\n",
      "5.038031\n",
      "5.9171658\n",
      "5.5631013\n",
      "6.4318185\n",
      "5.75839\n",
      "6.0534673\n",
      "6.5065517\n",
      "6.83752\n",
      "6.1085777\n",
      "6.0678167\n",
      "6.6217356\n",
      "6.0283756\n",
      "5.683641\n",
      "5.87008\n",
      "6.1534014\n",
      "6.277181\n",
      "6.506875\n",
      "6.255533\n",
      "6.447541\n",
      "5.796724\n",
      "6.3174987\n",
      "5.871937\n",
      "6.5182443\n",
      "6.298693\n",
      "5.6390886\n",
      "5.6051226\n",
      "6.307316\n",
      "6.798626\n",
      "5.8141375\n",
      "5.512777\n",
      "6.7988105\n",
      "5.3177238\n",
      "5.588767\n",
      "5.8966355\n",
      "6.3562045\n",
      "5.9142118\n",
      "5.471042\n",
      "6.287598\n",
      "6.1211324\n",
      "5.663477\n",
      "5.0906787\n",
      "5.5517044\n",
      "5.433024\n",
      "5.2843447\n",
      "5.4738784\n",
      "5.7923446\n",
      "6.3107166\n",
      "5.9082394\n",
      "5.4669185\n",
      "5.4259105\n",
      "5.504563\n",
      "6.0776\n",
      "5.034177\n",
      "6.3793535\n",
      "6.03976\n",
      "5.1344585\n",
      "5.5965405\n",
      "5.7623367\n",
      "5.71763\n",
      "5.5483212\n",
      "5.786848\n",
      "5.9117503\n",
      "5.5019965\n",
      "5.8820386\n",
      "5.8638372\n",
      "5.6979623\n",
      "5.5576773\n",
      "5.3351\n",
      "6.241429\n",
      "5.9065404\n",
      "6.39158\n",
      "5.6581974\n",
      "5.990926\n",
      "6.1336656\n",
      "6.1960735\n",
      "6.192605\n",
      "5.7304354\n",
      "6.1629276\n",
      "6.032074\n",
      "5.3876376\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of /home/weimin/seq2seq\\univariate_ts_model0 doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: /home/weimin; No such file or directory\n\t [[{{node save_1/SaveV2}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/SaveV2/tensor_names, save_1/SaveV2/shape_and_slices, Biases_out, Optimizer/OptimizeLoss/Biases_out/Adam, Optimizer/OptimizeLoss/Biases_out/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/bias/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1, Optimizer/OptimizeLoss/Weights_out/Adam, Optimizer/OptimizeLoss/Weights_out/Adam_1, Optimizer/OptimizeLoss/beta1_power, Optimizer/OptimizeLoss/beta2_power, Optimizer/OptimizeLoss/learning_rate, Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/bias, Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel, Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/bias, Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel, Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/bias, Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/kernel, Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/bias, Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/kernel, Weights_out, global_step)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1440\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: /home/weimin; No such file or directory\n\t [[node save_1/SaveV2 (defined at <ipython-input-45-e2a45659c2ba>:27)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/SaveV2/tensor_names, save_1/SaveV2/shape_and_slices, Biases_out, Optimizer/OptimizeLoss/Biases_out/Adam, Optimizer/OptimizeLoss/Biases_out/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/bias/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1, Optimizer/OptimizeLoss/Weights_out/Adam, Optimizer/OptimizeLoss/Weights_out/Adam_1, Optimizer/OptimizeLoss/beta1_power, Optimizer/OptimizeLoss/beta2_power, Optimizer/OptimizeLoss/learning_rate, Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/bias, Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel, Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/bias, Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel, Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/bias, Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/kernel, Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/bias, Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/kernel, Weights_out, global_step)]]\n\nCaused by op 'save_1/SaveV2', defined at:\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\asyncio\\base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\asyncio\\base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\asyncio\\events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"c:\\users\\stanl\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-45-e2a45659c2ba>\", line 27, in <module>\n    temp_saver = rnn_model['saver']()\n  File \"C:\\Users\\Stanl\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1102, in __init__\n    self.build()\n  File \"C:\\Users\\Stanl\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1114, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\Stanl\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1151, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\Stanl\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\training\\saver.py\", line 792, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\Stanl\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\training\\saver.py\", line 284, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\Stanl\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\training\\saver.py\", line 202, in save_op\n    tensors)\n  File \"C:\\Users\\Stanl\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1803, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"C:\\Users\\Stanl\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Stanl\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Stanl\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Stanl\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Failed to create a directory: /home/weimin; No such file or directory\n\t [[node save_1/SaveV2 (defined at <ipython-input-45-e2a45659c2ba>:27)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/SaveV2/tensor_names, save_1/SaveV2/shape_and_slices, Biases_out, Optimizer/OptimizeLoss/Biases_out/Adam, Optimizer/OptimizeLoss/Biases_out/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/bias/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam, Optimizer/OptimizeLoss/Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1, Optimizer/OptimizeLoss/Weights_out/Adam, Optimizer/OptimizeLoss/Weights_out/Adam_1, Optimizer/OptimizeLoss/beta1_power, Optimizer/OptimizeLoss/beta2_power, Optimizer/OptimizeLoss/learning_rate, Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/bias, Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel, Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/bias, Seq2seq/basic_rnn_seq2seq/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel, Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/bias, Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_0/lstm_cell/kernel, Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/bias, Seq2seq/basic_rnn_seq2seq/rnn_decoder/multi_rnn_cell/cell_1/lstm_cell/kernel, Weights_out, global_step)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-e2a45659c2ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mtemp_saver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'saver'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/weimin/seq2seq'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'univariate_ts_model0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Checkpoint saved at: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[0;32m   1457\u001b[0m                   save_path))\n\u001b[1;32m-> 1458\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Parent directory of /home/weimin/seq2seq\\univariate_ts_model0 doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "total_iteractions = 100\n",
    "batch_size = 16\n",
    "KEEP_RATE = 0.5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    " \n",
    "x = np.linspace(0, 30, 105)\n",
    "train_data_x = x[:85]\n",
    " \n",
    "rnn_model = build_graph(feed_previous=False)\n",
    " \n",
    "saver = tf.train.Saver()\n",
    " \n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    " \n",
    "    sess.run(init)\n",
    " \n",
    "    for i in range(total_iteractions):\n",
    "        batch_input, batch_output = generate_train_samples(batch_size=batch_size)\n",
    " \n",
    "        feed_dict = {rnn_model['enc_inp'][t]: batch_input[:,t].reshape(-1,input_dim) for t in range(input_seq_len)}\n",
    "        feed_dict.update({rnn_model['target_seq'][t]: batch_output[:,t].reshape(-1,output_dim) for t in range(output_seq_len)})\n",
    "        _, loss_t = sess.run([rnn_model['train_op'], rnn_model['loss']], feed_dict)\n",
    "        print(loss_t)\n",
    " \n",
    "    temp_saver = rnn_model['saver']()\n",
    "    save_path = temp_saver.save(sess, os.path.join('/home/weimin/seq2seq', 'univariate_ts_model0'))\n",
    " \n",
    "print(\"Checkpoint saved at: \", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
